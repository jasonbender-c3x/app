# The Prompt Life Cycle in a Production RAG System

The prompt life cycle in a Retrieval-Augmented Generation (RAG) system is a highly structured, real-time process orchestrated by a scalable backend service—typically Cloud Run—to ensure that the final response generated by the Large Language Model (LLM) is accurate and grounded in proprietary data.1
The life of a user query through the RAG system follows this comprehensive, multi-stage sequence:

The Prompt Life Cycle Walkthrough (RAG Retrieval)

User Query Reception and API GatewayThe interaction begins when the user submits a natural language query via the application's frontend interface. This request is securely routed to the Cloud Run service, which acts as the application's single API entry point and security gateway.1
State and History ManagementThe Cloud Run orchestrator identifies the active conversation session and retrieves the existing turn history from the persistence layer, such as Cloud Firestore.4 Critical logic is executed here to manage the finite context window of the LLM: older parts of the conversation might be summarized into long-term memory, while only the most recent turns are included as short-term memory, preventing token budget exhaustion while maintaining coherence.6
Query VectorizationThe user’s query (often combined with recent context from the history) is sent to the Vertex AI Text Embedding API.1 The API converts the text into a high-dimensional vector representation. This step is critical, as it must use the exact same embedding model and parameters that were used during the initial data ingestion phase to ensure the semantic space remains consistent.8
Retrieval (Vector Search)The generated query vector is passed to the Vertex AI Vector Search Index Endpoint.1 The index performs an efficient vector similarity search against the enterprise knowledge base, identifying the top $K$ most semantically relevant document chunks and retrieving their associated metadata (e.g., source URI, title, and section headers).10
Post-Retrieval OptimizationBefore context is injected, the Cloud Run orchestrator applies quality control. This step is essential for minimizing prompt length and maximizing relevance. Techniques include:
Deduplication: Identifying and removing redundant or highly overlapping chunks of text.6
Re-ranking: Using a secondary model or heuristic logic to prioritize the most relevant and novel chunks from the initial $K$ results.11
Context Augmentation (The Final Prompt Assembly)The finalized, refined set of retrieved context, along with the managed conversation history and a detailed set of guiding instructions (the System Prompt), are carefully concatenated to form the final, augmented prompt.2 This complete packet of information grounds the LLM, effectively minimizing the risk of factual hallucination.13
GenerationThe complete augmented prompt is submitted to the Gemini model deployed on Vertex AI. The model processes the prompt, reasoning over the provided context to generate a factually grounded response. To ensure low perceived latency, the API call must utilize the streaming method (e.g., generateContentStream).
Streaming and DeliveryThe generated response, along with the explicit groundingMetadata (which contains citation sources used by the model) , is immediately transmitted via the secure Cloud Run backend back to the client application.15 The frontend UI, often deployed using Firebase App Hosting, renders the response in real-time as the fragments arrive, providing an interactive and responsive user experience.16

Summary of the Runtime Component Chain


Works cited
RAG infrastructure for generative AI using Vertex AI and Vector Search | Cloud Architecture Center, accessed November 30, 2025, https://docs.cloud.google.com/architecture/gen-ai-rag-vertex-ai-vector-search
Vertex AI RAG Engine overview - Google Cloud Documentation, accessed November 30, 2025, https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview
Building RAG with Vertex AI RAG Engine | by Adityo Pratomo - Medium, accessed November 30, 2025, https://adityop.medium.com/building-rag-with-vertex-ai-rag-engine-e04bf9ebfa08
Build LLM-powered applications using LangChain | Firestore in Native mode, accessed November 30, 2025, https://docs.cloud.google.com/firestore/native/docs/langchain
Use Vertex AI RAG Engine in Gemini Live API - Google Cloud Documentation, accessed November 30, 2025, https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-rag-in-multimodal-live
Repeated Conversational Memory in RAG-based Chatbot - Google Help, accessed November 30, 2025, https://support.google.com/gemini/thread/376054018/repeated-conversational-memory-in-rag-based-chatbot?hl=en-gb
Context Window Management: Strategies for Long-Context AI Agents and Chatbots, accessed November 30, 2025, https://www.getmaxim.ai/articles/context-window-management-strategies-for-long-context-ai-agents-and-chatbots/
Understanding Embeddings for Generative AI - Unstructured, accessed November 30, 2025, https://unstructured.io/insights/understanding-embeddings-for-generative-ai
Streaming prediction with dataflow and vertex | Google Cloud Blog, accessed November 30, 2025, https://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex
Vector Search quickstart | Vertex AI - Google Cloud Documentation, accessed November 30, 2025, https://docs.cloud.google.com/vertex-ai/docs/vector-search/quickstart
LlamaIndex for RAG on Google Cloud, accessed November 30, 2025, https://cloud.google.com/blog/products/ai-machine-learning/llamaindex-for-rag-on-google-cloud
Structured model outputs - OpenAI API, accessed November 16, 2025, https://platform.openai.com/docs/guides/structured-outputs
Vertex AI RAG Engine: A developers tool, accessed November 30, 2025, https://developers.googleblog.com/en/vertex-ai-rag-engine-a-developers-tool/
What is Retrieval Augmented Generation (RAG)? - Databricks, accessed November 30, 2025, https://www.databricks.com/glossary/retrieval-augmented-generation-rag
Gemini Live API: Real-time AI for Manufacturing | Google Cloud Blog, accessed November 30, 2025, https://cloud.google.com/blog/topics/developers-practitioners/gemini-live-api-real-time-ai-for-manufacturing
Firebase App Hosting, accessed November 30, 2025, https://firebase.google.com/docs/app-hosting
RAG On GCP: Production-ready GenAI On Google Cloud Platform - Xebia, accessed November 30, 2025, https://xebia.com/blog/rag-on-gcp/
